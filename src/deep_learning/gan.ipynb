{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "def load_data():\n",
    "    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "    x_train = (x_train.astype(np.float32) - 127.5) / 127.5  # Normalize to [-1, 1]\n",
    "    x_train = np.expand_dims(x_train, axis=-1)  # Add channel dimension\n",
    "    return tf.data.Dataset.from_tensor_slices(x_train).shuffle(60000).batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Build generator\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(7*7*256, use_bias=False, input_shape=(100,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Reshape((7, 7, 256)),\n",
    "        layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', activation='tanh') # [-1, 1]\n",
    "    ])\n",
    "    return model\n",
    "#Batch Normalization normalizes activations of previous layer at each batch, i.e helps stabilize and speed up training, \n",
    "\n",
    "# Build discriminator\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv2D(64, (5, 5), strides=2, padding='same', input_shape=[28, 28, 1]),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Conv2D(128, (5, 5), strides=2, padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Optimizers\n",
    "def get_optimizers():\n",
    "    return tf.keras.optimizers.Adam(1e-4), tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "g_opt, d_opt = get_optimizers()\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "#   Discrimator wants real image as 1 and fake as 0\n",
    "# Generator wants discriminator to think fake images are real (1)\n",
    "\n",
    "# Training step\n",
    "@tf.function  # Compiles to a TensorFlow graph for speed\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([128, 100])  # creates a batch of 128 random noise vectors, each of size 100-dimensional\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: #recording operations for automatic differentiation\n",
    "        generated_images = generator(noise, training=True)  # generator takes the random noise and produces fake MNIST-like images resembling real MNIST digits\n",
    "        real_output = discriminator(images, training=True)  # discriminator evaluates real MNIST images are real (should be close to 1)\n",
    "        fake_output = discriminator(generated_images, training=True)  # discriminator predicts whether the generated  fake images are real or fake (should be close to 0)\n",
    "\n",
    "        gen_loss = loss_fn(tf.ones_like(fake_output), fake_output)  # tf.ones_like creates a tensor of ones representing real labels\n",
    "                                                                    # loss_fn (Binary crossentropy loss ) how close fake output is to 1\n",
    "                                                                    # lower generator loss means the generator is successfully fooling the discriminator\n",
    "                                                                    # higher generator loss means the generator is failing to fool the discriminator\n",
    "                                                                    \n",
    "        disc_loss = (loss_fn(tf.ones_like(real_output), real_output) + # how well discriminator classifies real images as real (close to 1)\n",
    "                            loss_fn(tf.zeros_like(fake_output), fake_output)) # how well discriminator classifies fake images as fake (close to 0) - same as gen_loss\n",
    "                                                                    # lower discriminator loss means it is successfully distinguishing real from fake \n",
    "    # gradient calculation and weight updates with the tape\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables) # compute gradients for generator parameters\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables) # compute gradients for discriminator parameters\n",
    "    # optimizer apply gradients\n",
    "    g_opt.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))# update generator weights (when we do zip, zip means we are computing one by one and updating)\n",
    "    d_opt.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables)) # update discriminator weight\n",
    "    \n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# Training loop\n",
    "def train(dataset, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataset:\n",
    "            gen_loss, disc_loss = train_step(batch)\n",
    "        print(f\"Epoch {epoch+1}, Gen Loss: {gen_loss.numpy():.4f}, Disc Loss: {disc_loss.numpy():.4f}\")\n",
    "\n",
    "# Function to generate and display sample images from generator\n",
    "def generate_and_show():\n",
    "    noise = tf.random.normal([16, 100])  # Generate 16 noise vectors\n",
    "    images = generator(noise, training=False)  # Generate fake images\n",
    "    images = (images + 1) / 2  # Rescale from [-1, 1] to [0, 1] for display\n",
    "\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(4, 4))  # Create 4x4 grid\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i, :, :, 0], cmap='gray')  # Show single channel grayscale image\n",
    "        ax.axis('off')  # Hide axis\n",
    "    plt.show()\n",
    "\n",
    "# Load the dataset and begin training\n",
    "mnist_data = load_data()\n",
    "train(mnist_data, epochs=10)\n",
    "generate_and_show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e5ae5",
   "metadata": {},
   "source": [
    "##  GAN Code Concepts and Explanations\n",
    "\n",
    "---\n",
    "\n",
    "###  `.batch(128)`\n",
    "- **Description:** Groups the data into batches of 128 samples for efficient processing.\n",
    "- **Why:** Processes multiple samples together (instead of one at a time), improving performance.\n",
    "\n",
    "###  `.prefetch(tf.data.AUTOTUNE)`\n",
    "- **Description:** Optimizes data loading by prefetching batches while the model is training.\n",
    "- **Why:** `AUTOTUNE` lets TensorFlow choose the optimal prefetch size based on system resources.\n",
    "\n",
    "### âš ï¸ Discriminator Overpowering Warning\n",
    "- If **discriminator loss > 1.2**, it may overpower the generator, preventing effective generator training.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§µ `@tf.function` Decorator\n",
    "\n",
    "**Purpose:** Converts Python functions to TensorFlow graphs for faster, optimized execution.\n",
    "\n",
    "**Benefits:**\n",
    "1. Optimized GPU operations.\n",
    "2. Avoids Python overhead by compiling to low-level operations.\n",
    "3. Allows parallel execution.\n",
    "\n",
    "### Why It Matters:\n",
    "Improves training efficiency in deep learning, especially for complex models like GANs.\n",
    "\n",
    "---\n",
    "\n",
    "### Training Phases in GANs\n",
    "\n",
    "- **Early Training:**  \n",
    "  - Generator is weak: `fake_output` is low â†’ `gen_loss` is high.  \n",
    "  - Discriminator is strong â†’ `disc_loss` is low.\n",
    "\n",
    "- **Mid Training:**  \n",
    "  - Generator improves â†’ `fake_output` increases â†’ `gen_loss` drops.  \n",
    "  - Discriminator gets confused â†’ `disc_loss` increases.\n",
    "\n",
    "- **Late Training (Ideal Case):**  \n",
    "  - Generator is strong: `fake_output â‰ˆ real_output`.  \n",
    "  - Discriminator struggles â†’ `gen_loss` and `disc_loss` are balanced.\n",
    "\n",
    "---\n",
    "\n",
    "###  `tf.GradientTape()`\n",
    "\n",
    "**Purpose:** Enables automatic differentiation and gradient computation.\n",
    "\n",
    "### How It Works:\n",
    "1. **Record operations:** Wrap code in `with tf.GradientTape() as tape` to track computations.\n",
    "2. **Compute gradients:** Use `tape.gradient(loss, model.trainable_variables)` after loss calculation.\n",
    "3. **Apply gradients:** Use an optimizer (e.g., `Adam`) to update model weights.\n",
    "\n",
    "---\n",
    "\n",
    "###  `.shuffle(60000)`\n",
    "- **Purpose:** Randomly shuffles the dataset with a buffer size of 60,000.\n",
    "- **Why:** Helps the model generalize better by mixing samples.\n",
    "\n",
    "---\n",
    "\n",
    "###  Summary\n",
    "Use these techniques to:\n",
    "- Improve GAN training efficiency.\n",
    "- Prevent imbalance between generator and discriminator.\n",
    "- Utilize TensorFlow's performance-boosting features.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
